{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM,Conv2D, MaxPool2D, BatchNormalization,Dropout, Embedding\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from vgg_base import get_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "with open(os.getcwd() + '/CUHK-PEDES/CUHK-PEDES/caption_all.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "capts=[]\n",
    "img_paths=[]\n",
    "for val in data:\n",
    "    cap=val['captions']\n",
    "    temp=\" \".join(cap)\n",
    "    if len(temp.split())<100:\n",
    "        capts.append(temp)\n",
    "        img_paths.append(os.getcwd() + '/CUHK-PEDES/CUHK-PEDES/imgs/'+val['file_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs=['UNK','PAD']\n",
    "counts=dict(Counter(\" \".join(capts).split(' ')))\n",
    "for w, c in zip(counts.keys(), counts.values()):\n",
    "    if c>50:vocabs.append(w)\n",
    "int2vocab=dict(enumerate(vocabs))\n",
    "vocab2int={i:c for i, c in zip(int2vocab.values(), int2vocab.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_ids=[[vocab2int.get(word, 0) for word in val.split()]for val in capts]\n",
    "caption_ids_pad=tf.keras.preprocessing.sequence.pad_sequences(caption_ids, padding='post', value=vocab2int['PAD'])\n",
    "#img_paths=[]\n",
    "#for val in data:\n",
    " #   img_paths.append(os.getcwd() + '/CUHK-PEDES/CUHK-PEDES/imgs/'+val['file_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "num_units=15\n",
    "batch_size=10\n",
    "vocab_size=len(vocab2int)\n",
    "max_len=99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#bce = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction='none')\n",
    "#real = np.array([[[1., 0., 0.], [0., 1., 0], [0., 0., 1.]], [[0., 1., 0.], [0., 0., 1.], [1., 0., 0.]]])\n",
    "#pred= np.array([[[0.7,0.3, 0.1], [0.5,0.6,0.2], [0.4,0.3,0.8],[1,2,3]], [[0.9,0.6,0.7],[0.5,0.3,0.8],[0.1,0.5,0.6],[2,3,4]]])\n",
    "#loss=bce(real, pred)\n",
    "#print('Loss: ', tf.reduce_sum(loss, axis=1) )  # Loss: 11.5228\n",
    "#p  = tf.concat([real,pred], axis=1)\n",
    "#print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_train, img_val, capt_train, capt_val = train_test_split(img_paths, caption_ids_pad, test_size=0.6)\n",
    "img_train_true = img_train[:len(img_train)//2]\n",
    "\n",
    "img_train_false = img_train[len(img_train)//2:]\n",
    "img_train_false = shuffle(img_train_false)\n",
    "\n",
    "img_train_final=img_train_true+img_train_false\n",
    "true_false_labels = [[1]*(len(img_train_true)) + [0]*(len(img_train_true))]\n",
    "true_false_labels = np.array(true_false_labels).ravel()\n",
    "\n",
    "img_train_temp = img_train_final[:250] + img_train_final[len(img_train_final)-250:]\n",
    "capt_temp = np.append(capt_train[:250], capt_train[len(capt_train)-250:], axis=0)\n",
    "labels_temp=np.append(true_false_labels[:250], true_false_labels[len(true_false_labels)-250:], axis=0)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_train_temp, capt_temp, labels_temp)).shuffle(len(img_train_temp))\n",
    "dataset=dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass vgg_base(tf.keras.Model):\\n    def __init__(self):\\n        super(vgg_base, self).__init__()\\n        self.conv64_2=tf.keras.Sequential([Conv2D(64, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\\n                                           Dropout(rate=0.2),\\n                                       Conv2D(64, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\\n                                            Dropout(rate=0.3),\\n                                          MaxPool2D()])\\n        self.conv128_2=tf.keras.Sequential([Conv2D(128, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\\n                                            Dropout(rate=0.2),\\n                                       Conv2D(128, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\\n                                             Dropout(rate=0.3),\\n                                          MaxPool2D()])\\n        self.conv256_3=tf.keras.Sequential([Conv2D(256, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\\n                                       Conv2D(256, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\\n                                            Dropout(rate=0.4),\\n                                        Conv2D(256, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\\n                                       MaxPool2D()])\\n        self.conv512_3_1=tf.keras.Sequential([Conv2D(512, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\\n                                       Conv2D(512, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\\n                                        Dropout(rate=0.6),\\n                                        Conv2D(512, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\\n                                       MaxPool2D()])\\n        self.conv512_3_2=tf.keras.Sequential([Conv2D(512, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\\n                                       Conv2D(512, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\\n                                        Dropout(rate=0.7),\\n                                        Conv2D(512, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\\n                                       MaxPool2D()])\\n        self.dropout1 = Dropout(rate=0.3)\\n        self.dropout2 = Dropout(rate=0.3)\\n        self.dropout3 = Dropout(rate=0.4)\\n        self.fc1=Dense(256)\\n        self.fc2=Dense(512)\\n        self.fc3=Dense(1024)\\n        self.fc =Dense(199)\\n        self.fc4=Dense(2500)\\n\\n        \\n    def call(self, x):\\n        #print('running CNN..')\\n        x=self.conv64_2(x)\\n        #print(x.shape)\\n        x=self.conv128_2(x)\\n        #print(x.shape)\\n        x=self.conv256_3(x)\\n        #print(x.shape)\\n        #x=self.conv512_3_1(x)\\n        #x=self.conv512_3_2(x)\\n        fc1_=self.fc1(x)\\n        fc1_=self.dropout1(fc1_)\\n        fc2_=self.fc2(fc1_)\\n        fc2_=self.dropout2(fc2_)\\n        fc3_=self.fc3(fc2_)\\n        fc3_=self.dropout3(fc3_)\\n        fc4_=self.fc4(fc3_)\\n        #print(fc2_.shape)\\n        fc4=tf.nn.softmax(self.fc(fc3_))\\n        \\n        return fc4_, fc4\\n         \\nvgg= vgg_base()\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#custom vgg\n",
    "'''\n",
    "class vgg_base(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(vgg_base, self).__init__()\n",
    "        self.conv64_2=tf.keras.Sequential([Conv2D(64, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\n",
    "                                           Dropout(rate=0.2),\n",
    "                                       Conv2D(64, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\n",
    "                                            Dropout(rate=0.3),\n",
    "                                          MaxPool2D()])\n",
    "        self.conv128_2=tf.keras.Sequential([Conv2D(128, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\n",
    "                                            Dropout(rate=0.2),\n",
    "                                       Conv2D(128, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\n",
    "                                             Dropout(rate=0.3),\n",
    "                                          MaxPool2D()])\n",
    "        self.conv256_3=tf.keras.Sequential([Conv2D(256, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\n",
    "                                       Conv2D(256, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\n",
    "                                            Dropout(rate=0.4),\n",
    "                                        Conv2D(256, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\n",
    "                                       MaxPool2D()])\n",
    "        self.conv512_3_1=tf.keras.Sequential([Conv2D(512, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\n",
    "                                       Conv2D(512, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\n",
    "                                        Dropout(rate=0.6),\n",
    "                                        Conv2D(512, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\n",
    "                                       MaxPool2D()])\n",
    "        self.conv512_3_2=tf.keras.Sequential([Conv2D(512, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\n",
    "                                       Conv2D(512, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\n",
    "                                        Dropout(rate=0.7),\n",
    "                                        Conv2D(512, kernel_size=(3,3), strides=1, activation=tf.nn.relu),\n",
    "                                       MaxPool2D()])\n",
    "        self.dropout1 = Dropout(rate=0.3)\n",
    "        self.dropout2 = Dropout(rate=0.3)\n",
    "        self.dropout3 = Dropout(rate=0.4)\n",
    "        self.fc1=Dense(256)\n",
    "        self.fc2=Dense(512)\n",
    "        self.fc3=Dense(1024)\n",
    "        self.fc =Dense(199)\n",
    "        self.fc4=Dense(2500)\n",
    "\n",
    "        \n",
    "    def call(self, x):\n",
    "        #print('running CNN..')\n",
    "        x=self.conv64_2(x)\n",
    "        #print(x.shape)\n",
    "        x=self.conv128_2(x)\n",
    "        #print(x.shape)\n",
    "        x=self.conv256_3(x)\n",
    "        #print(x.shape)\n",
    "        #x=self.conv512_3_1(x)\n",
    "        #x=self.conv512_3_2(x)\n",
    "        fc1_=self.fc1(x)\n",
    "        fc1_=self.dropout1(fc1_)\n",
    "        fc2_=self.fc2(fc1_)\n",
    "        fc2_=self.dropout2(fc2_)\n",
    "        fc3_=self.fc3(fc2_)\n",
    "        fc3_=self.dropout3(fc3_)\n",
    "        fc4_=self.fc4(fc3_)\n",
    "        #print(fc2_.shape)\n",
    "        fc4=tf.nn.softmax(self.fc(fc3_))\n",
    "        \n",
    "        return fc4_, fc4\n",
    "         \n",
    "vgg= vgg_base()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "\n",
    "# define input layer\n",
    "input_layer = tf.keras.layers.Input([224, 224, 3])\n",
    "\n",
    "red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=input_layer)\n",
    "bgr = tf.concat(axis=3, values=[blue - VGG_MEAN[0], green - VGG_MEAN[1], red - VGG_MEAN[2]])\n",
    "\n",
    "# Block 1\n",
    "conv1_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=[3, 3], strides=[1, 1], padding='same',\n",
    "                                 use_bias=True, activation='relu', name='conv1_1')(bgr)\n",
    "\n",
    "conv1_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=[3, 3], strides=[1, 1], padding='same',\n",
    "                                 use_bias=True, activation='relu', name='conv1_2')(conv1_1)\n",
    "pool1_1 = tf.nn.max_pool(conv1_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1_1')\n",
    "\n",
    "# Block 2\n",
    "conv2_1 = tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], padding='same',\n",
    "                                 use_bias=True, activation='relu', name='conv2_1')(pool1_1)\n",
    "conv2_2 = tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], padding='same',\n",
    "                                 use_bias=True, activation='relu', name='conv2_2')(conv2_1)\n",
    "pool2_1 = tf.nn.max_pool(conv2_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool2_1')\n",
    "\n",
    "# Block 3\n",
    "conv3_1 = tf.keras.layers.Conv2D(filters=256, kernel_size=[3, 3], strides=[1, 1], padding='same',\n",
    "                                 use_bias=True, activation='relu', name='conv3_1')(pool2_1)\n",
    "conv3_2 = tf.keras.layers.Conv2D(filters=256, kernel_size=[3, 3], strides=[1, 1], padding='same',\n",
    "                                 use_bias=True, activation='relu', name='conv3_2')(conv3_1)\n",
    "conv3_3 = tf.keras.layers.Conv2D(filters=256, kernel_size=[3, 3], strides=[1, 1], padding='same',\n",
    "                                 use_bias=True, activation='relu', name='conv3_3')(conv3_2)\n",
    "pool3_1 = tf.nn.max_pool(conv3_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool3_1')\n",
    "\n",
    "# Block 4\n",
    "conv4_1 = tf.keras.layers.Conv2D(filters=512, kernel_size=[3, 3], strides=[1, 1], padding='same',\n",
    "                                 use_bias=True, activation='relu', name='conv4_1')(pool3_1)\n",
    "conv4_2 = tf.keras.layers.Conv2D(filters=512, kernel_size=[3, 3], strides=[1, 1], padding='same',\n",
    "                                 use_bias=True, activation='relu', name='conv4_2')(conv4_1)\n",
    "conv4_3 = tf.keras.layers.Conv2D(filters=512, kernel_size=[3, 3], strides=[1, 1], padding='same',\n",
    "                                 use_bias=True, activation='relu', name='conv4_3')(conv4_2)\n",
    "pool4_1 = tf.nn.max_pool(conv4_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool4_1')\n",
    "\n",
    "# Block 4\n",
    "conv5_1 = tf.keras.layers.Conv2D(filters=512, kernel_size=[3, 3], strides=[1, 1], padding='same',\n",
    "                                 use_bias=True, activation='relu', name='conv5_1')(pool4_1)\n",
    "conv5_2 = tf.keras.layers.Conv2D(filters=512, kernel_size=[3, 3], strides=[1, 1], padding='same',\n",
    "                                 use_bias=True, activation='relu', name='conv5_2')(conv5_1)\n",
    "conv5_3 = tf.keras.layers.Conv2D(filters=512, kernel_size=[3, 3], strides=[1, 1], padding='same',\n",
    "                                 use_bias=True, activation='relu', name='conv5_3')(conv5_2)\n",
    "pool5_1 = tf.nn.max_pool(conv5_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool5_1')\n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(pool5_1)\n",
    "fc6 = tf.keras.layers.Dense(units=4096, use_bias=True, name='fc6', activation='relu')(flatten)\n",
    "fc7 = tf.keras.layers.Dense(units=4096, use_bias=True, name='fc7', activation='relu')(fc6)\n",
    "fc8 = tf.keras.layers.Dense(units=1000, use_bias=True, name='fc8', activation=None)(fc7)\n",
    "\n",
    "\n",
    "#prob = tf.nn.softmax(fc8)\n",
    "\n",
    "# Build model\n",
    "vgg = tf.keras.Model(input_layer, fc8)\n",
    "weighs = np.load(\"./vgg16.npy\", encoding='latin1', allow_pickle=True).item()\n",
    "for layer_name in weighs.keys():\n",
    "    layer = vgg.get_layer(layer_name)\n",
    "    layer.set_weights(weighs[layer_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(tf.keras.Model):\n",
    "    def __init__(self, num_units, batch_size, max_len):\n",
    "        super(model, self).__init__()\n",
    "        self.num_units=num_units\n",
    "        self.max_len=max_len\n",
    "        self.batch_size=batch_size\n",
    "        self.lstm=LSTM(num_units, return_sequences=True, return_state=True)\n",
    "        self.fc1=Dense(self.max_len, activation='relu')\n",
    "        self.fc2=Dense(self.max_len)\n",
    "        self.capt_fc=Dense(32)\n",
    "        self.img_fc1=Dense(512)\n",
    "        self.img_fc2=Dense(512)\n",
    "        self.word_fc=Dense(self.max_len, activation='relu')\n",
    "        \n",
    "    def call(self, x, img_batch, state):\n",
    "        capt = self.capt_fc(x) #bsxmaxlenx15\n",
    "        img_features = vgg(img_batch)#bsx1000\n",
    "        im_fc1=self.img_fc1(img_features)\n",
    "        im_fc2=self.img_fc2(im_fc1) #bsx512\n",
    "        img_fc=tf.concat([im_fc1, im_fc2], axis=1) #bsx1024\n",
    "        vis_units=tf.reshape(img_fc, shape=(batch_size, 32, 32))\n",
    "        im_exp=tf.expand_dims(im_fc2, axis=1) #bsx1x512\n",
    "        #im_per_word=tf.keras.backend.repeat_elements(im_exp, max_len, axis=1) #bsxmaxlenx512\n",
    "        input_concat = tf.concat([capt, vis_units], 1) #bsx1024x512\n",
    "        out, state1, state2=self.lstm(input_concat) \n",
    "        state_concat=tf.concat([state1, state2], axis=1) #bsx2*numunit\n",
    "        \n",
    "        #state_expand=tf.expand_dims(state_concat, axis=1) #bsx1x 2*num\n",
    "        #state_per_word=tf.keras.backend.repeat_elements(state_expand, max_len, axis=1) #bsxmaxlenx2*num\n",
    "        \n",
    "        word_gate=self.word_fc(state_concat)\n",
    "        word_gate=tf.nn.sigmoid(word_gate)\n",
    "        word_gate=tf.expand_dims(word_gate, axis=2) #bsxmaxlenx1\n",
    "        \n",
    "        attention=tf.nn.relu(self.fc1(state_concat)) #bsxmaxlen\n",
    "        score=tf.nn.softmax(attention) #bsxmaxlenx1 \n",
    "        score=tf.expand_dims(score, axis=2)\n",
    "        \n",
    "        img_softmax=tf.nn.softmax(im_exp, axis=2) #bsx1x512\n",
    "        \n",
    "        word_img_affinity = word_gate*score*img_softmax #bsxmaxlenx512\n",
    "        sentence_level_affinity=tf.nn.softmax(tf.reduce_sum(tf.reduce_sum(word_img_affinity, axis=2), axis=1))\n",
    "        \n",
    "        return (state1, state2), sentence_level_affinity\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return (tf.zeros((self.batch_size, self.num_units)), tf.zeros((self.batch_size, self.num_units)))\n",
    "    \n",
    "network = model(num_units, batch_size, max_len)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndatas=next(dataset.__iter__())\\nimg_arr=[]\\ncapts = tf.one_hot(datas[1], depth=vocab_size, dtype=tf.int32)\\nfor path in datas[0]:img_arr.append(load_image(path.numpy()))\\nimg_arr = tf.convert_to_tensor(img_arr)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample checkings\n",
    "'''\n",
    "datas=next(dataset.__iter__())\n",
    "img_arr=[]\n",
    "capts = tf.one_hot(datas[1], depth=vocab_size, dtype=tf.int32)\n",
    "for path in datas[0]:img_arr.append(load_image(path.numpy()))\n",
    "img_arr = tf.convert_to_tensor(img_arr)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optm=tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_object=tf.losses.BinaryCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    #mask=tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_=loss_object(real, pred)\n",
    "    #mask_=tf.cast(mask, dtype=loss_.dtype)\n",
    "    #loss_*=mask_\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.resize(img, (224,224))\n",
    "    return img/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(optm, img_batch, capt_batch, labels, states):\n",
    "    with tf.GradientTape() as tape:\n",
    "        states, affinity = network(capt_batch, img_batch, states)\n",
    "        loss = loss_function(labels, affinity)\n",
    "        variables = network.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optm.apply_gradients(zip(gradients, variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1  batch : 10 loss : 0.7144\n",
      "epoch : 1  batch : 20 loss : 0.6944\n",
      "epoch : 1  batch : 30 loss : 0.7044\n",
      "epoch : 1  batch : 40 loss : 0.6744\n",
      "epoch : 1  batch : 50 loss : 0.6944\n",
      "\n",
      "\n",
      "epoch : 1 loss : 0.6944\n",
      "epoch : 2  batch : 10 loss : 0.7044\n",
      "epoch : 2  batch : 20 loss : 0.7044\n",
      "epoch : 2  batch : 30 loss : 0.6944\n",
      "epoch : 2  batch : 40 loss : 0.6744\n",
      "epoch : 2  batch : 50 loss : 0.6444\n",
      "\n",
      "\n",
      "epoch : 2 loss : 0.6944\n",
      "epoch : 3  batch : 10 loss : 0.7044\n",
      "epoch : 3  batch : 20 loss : 0.7044\n",
      "epoch : 3  batch : 30 loss : 0.6944\n",
      "epoch : 3  batch : 40 loss : 0.7044\n",
      "epoch : 3  batch : 50 loss : 0.6644\n",
      "\n",
      "\n",
      "epoch : 3 loss : 0.6944\n",
      "epoch : 4  batch : 10 loss : 0.6944\n",
      "epoch : 4  batch : 20 loss : 0.6944\n",
      "epoch : 4  batch : 30 loss : 0.7144\n",
      "epoch : 4  batch : 40 loss : 0.6744\n",
      "epoch : 4  batch : 50 loss : 0.7144\n",
      "\n",
      "\n",
      "epoch : 4 loss : 0.6944\n",
      "epoch : 5  batch : 10 loss : 0.6944\n",
      "epoch : 5  batch : 20 loss : 0.7144\n",
      "epoch : 5  batch : 30 loss : 0.7144\n",
      "epoch : 5  batch : 40 loss : 0.6944\n",
      "epoch : 5  batch : 50 loss : 0.7044\n",
      "\n",
      "\n",
      "epoch : 5 loss : 0.6944\n",
      "epoch : 6  batch : 10 loss : 0.7044\n",
      "epoch : 6  batch : 20 loss : 0.6844\n",
      "epoch : 6  batch : 30 loss : 0.6944\n",
      "epoch : 6  batch : 40 loss : 0.6844\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-86876f3ca739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_path_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapt_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mimg_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg_path_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimg_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mimg_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcapt_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapt_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-1edad0979c82>\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36mresize_images_v2\u001b[0;34m(images, size, method, preserve_aspect_ratio, antialias, name)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mpreserve_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreserve_aspect_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m       skip_resize_if_same=False)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36m_resize_images_common\u001b[0;34m(images, resizer_fn, size, preserve_aspect_ratio, name, skip_resize_if_same)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m       \u001b[0mis_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\'images\\' must have either 3 or 4 dimensions.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/miniconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mexpand_dims\u001b[0;34m(input, axis, name, dim)\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Must specify an axis argument to tf.expand_dims()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mexpand_dims_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mexpand_dims_v2\u001b[0;34m(input, axis, name)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0mdimension\u001b[0m \u001b[0mof\u001b[0m \u001b[0msize\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0madded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m   \"\"\"\n\u001b[0;32m--> 325\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mexpand_dims\u001b[0;34m(input, axis, name)\u001b[0m\n\u001b[1;32m   2447\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   2448\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2449\u001b[0;31m         \"ExpandDims\", name, _ctx._post_execution_callbacks, input, axis)\n\u001b[0m\u001b[1;32m   2450\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2451\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    initial_state = network.get_initial_state()\n",
    "    total_loss=0\n",
    "    for batch, (img_path_batch, capt_batch, labels) in enumerate(dataset):\n",
    "        img_batch=[]\n",
    "        for img in img_path_batch:img_batch.append(load_image(img.numpy()))\n",
    "        img_batch=tf.convert_to_tensor(img_batch, dtype=tf.float32)\n",
    "        capt_batch=tf.one_hot(capt_batch, depth=vocab_size)\n",
    "        cur_loss = train_step(optm,img_batch, capt_batch, labels, initial_state )\n",
    "        total_loss+=cur_loss\n",
    "        \n",
    "        if (batch+1)%10==0:\n",
    "            print('epoch : {}  batch : {} loss : {:.4f}'.format(epoch+1, batch+1, cur_loss))\n",
    "            \n",
    "    if (epoch+1)%1==0:\n",
    "        print('\\n')\n",
    "        print('epoch : {} loss : {:.4f}'.format(epoch+1, total_loss/(len(img_train_temp)/batch_size )))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO-DO loss is varying with num_units others remaining constant. with 500 samples  loss -> 0.69 - 0.65 in 20 epoch with 15 units\n",
    "#train model with full samples "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
